{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Word: matematyka, Emb.shape=(768,)\n",
      "Word: długopis, Emb.shape=(768,)\n",
      "Word: bławatki, Emb.shape=(768,)\n",
      "Word: szarlotka, Emb.shape=(768,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = 'flax-community/papuGaPT2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "tokenizer_papuga = AutoTokenizer.from_pretrained(model_name)\n",
    "model_papuga = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# The embedding matrix for PapugaGPT2 is at:\n",
    "# model_papuga.transformer.wte.weight (shape: [vocab_size, embedding_dim])\n",
    "papuga_emb_matrix = model_papuga.transformer.wte.weight.detach().cpu().numpy()\n",
    "\n",
    "def get_word_embedding_papuga_non_contextual(word: str):\n",
    "    \"\"\"\n",
    "    Returns a single vector representing the word by averaging\n",
    "    the embeddings of all tokens that make up the word.\n",
    "    \"\"\"\n",
    "    # Encode the word\n",
    "    # PapuGaGPT2 expects a leading space for 'proper' tokenization, so let's add one:\n",
    "    input_ids = tokenizer_papuga(\" \"+word, add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    # Retrieve each token's embedding from the embedding matrix\n",
    "    token_vectors = []\n",
    "    for tid in input_ids:\n",
    "        token_vectors.append(papuga_emb_matrix[tid])\n",
    "    \n",
    "    # Average them to get a single embedding\n",
    "    # return np.mean(token_vectors, axis=0)\n",
    "    \n",
    "    # return the max value of each dimension\n",
    "    return np.max(token_vectors, axis=0)\n",
    "\n",
    "# Example usage:\n",
    "words = [\"matematyka\", \"długopis\", \"bławatki\", \"szarlotka\"]\n",
    "for w in words:\n",
    "    emb = get_word_embedding_papuga_non_contextual(w)\n",
    "    print(f\"Word: {w}, Emb.shape={emb.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.sso.sso_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Word: długopis, Emb.shape=(768,)\n",
      "BERT Word: szarlotka, Emb.shape=(768,)\n",
      "BERT Word: bławatki, Emb.shape=(768,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name_herbert = \"allegro/herbert-base-cased\"\n",
    "tokenizer_herbert = AutoTokenizer.from_pretrained(model_name_herbert)\n",
    "model_herbert = AutoModel.from_pretrained(model_name_herbert).to(device)\n",
    "\n",
    "def get_word_embedding_bert_contextual(word: str):\n",
    "    \"\"\"\n",
    "    Gets the 'contextual' BERT-based embedding for a single word.\n",
    "    By default, we use the last hidden state and average over the tokens that form the word.\n",
    "    \"\"\"\n",
    "    # Option 1: Just feed the single word (might be suboptimal, no real context).\n",
    "    # Option 2: Insert the word in a minimal sentence, e.g. \"To jest WORD.\"\n",
    "    # We'll show option 1 for simplicity:\n",
    "    input_ids = tokenizer_herbert.encode(word, add_special_tokens=True)\n",
    "    # Convert to tensors\n",
    "    input_ids_tensor = torch.tensor([input_ids]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_herbert(input_ids_tensor)\n",
    "    \n",
    "    # 'outputs' is a BaseModelOutput with the first element: last_hidden_state shape => [batch_size, seq_len, hidden_dim]\n",
    "    last_hidden_state = outputs.last_hidden_state.squeeze(0)  # shape => [seq_len, hidden_dim]\n",
    "    \n",
    "    # The word may be split into multiple subwords (HerBERT uses WordPiece).\n",
    "    # Typically, [CLS] is the first token. We'll skip that in the averaging.\n",
    "    # Also skip [SEP] if present at the end.\n",
    "    # So we'll average over indices [1 : seq_len-1] ignoring special tokens\n",
    "    # If the word is subword-split, this includes all subword tokens.\n",
    "\n",
    "    # indices 1 to len(input_ids)-2 to skip [CLS] and [SEP].\n",
    "    subword_vectors = last_hidden_state[1:-1]\n",
    "    # subword_vectors = last_hidden_state\n",
    "    if subword_vectors.shape[0] == 0:\n",
    "        # edge case if the word is extremely short, fallback\n",
    "        subword_vectors = last_hidden_state\n",
    "\n",
    "    # average pooling\n",
    "    word_vector = torch.mean(subword_vectors, dim=0)\n",
    "\n",
    "    return word_vector.cpu().numpy()\n",
    "\n",
    "# Example usage:\n",
    "words_bert = [\"długopis\", \"szarlotka\", \"bławatki\"]\n",
    "for w in words_bert:\n",
    "    emb_bert = get_word_embedding_bert_contextual(w)\n",
    "    print(f\"BERT Word: {w}, Emb.shape={emb_bert.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "długopis -> dulgopis\n",
      "krówka -> krkwoa\n",
      "źdźbło -> zdzblo\n",
      "pająk -> paajk\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Polish diacritics mapping\n",
    "polish_diacritics_map = {\n",
    "    'ą': 'a', 'ć': 'c', 'ę': 'e', 'ł': 'l', \n",
    "    'ń': 'n', 'ó': 'o', 'ś': 's', 'ź': 'z', 'ż': 'z'\n",
    "}\n",
    "\n",
    "def remove_polish_diacritics(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes Polish diacritics by mapping each character\n",
    "    to its unaccented counterpart.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for ch in word:\n",
    "        if ch in polish_diacritics_map:\n",
    "            output.append(polish_diacritics_map[ch])\n",
    "        else:\n",
    "            output.append(ch)\n",
    "    return \"\".join(output)\n",
    "\n",
    "def random_swap(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Swaps two letters in the word.\n",
    "    If the word has length < 2, return it as is.\n",
    "    \"\"\"\n",
    "    w = list(word)\n",
    "    if len(w) < 2:\n",
    "        return word\n",
    "    \n",
    "    # pick two positions to swap\n",
    "    i = random.randint(0, len(w) - 1)\n",
    "    j = random.randint(0, len(w) - 1)\n",
    "    \n",
    "    # ensure we do something \n",
    "    if i != j:\n",
    "        w[i], w[j] = w[j], w[i]\n",
    "    return \"\".join(w)\n",
    "\n",
    "def distort_word(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Applies both distortions:\n",
    "    1) Removing Polish diacritics\n",
    "    2) Random swapping of two letters\n",
    "    ensuring the final word is *always* different in some way.\n",
    "    \"\"\"\n",
    "    # remove diacritics\n",
    "    no_diac = remove_polish_diacritics(word)\n",
    "    # random swap\n",
    "    distorted = random_swap(no_diac)\n",
    "    return distorted\n",
    "\n",
    "# Example\n",
    "original_words = [\"długopis\", \"krówka\", \"źdźbło\", \"pająk\"]\n",
    "for w in original_words:\n",
    "    dist = distort_word(w)\n",
    "    print(f\"{w} -> {dist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_txt = '''\n",
    "piśmiennicze: pisak flamaster ołówek długopis pióro\n",
    "małe_ssaki: mysz szczur chomik łasica kuna bóbr\n",
    "okręty: niszczyciel lotniskowiec trałowiec krążownik pancernik fregata korweta\n",
    "lekarze: lekarz pediatra ginekolog kardiolog internista geriatra\n",
    "zupy: rosół żurek barszcz\n",
    "uczucia: miłość przyjaźń nienawiść gniew smutek radość strach\n",
    "działy_matematyki: algebra analiza topologia logika geometria \n",
    "budynki_sakralne: kościół bazylika kaplica katedra świątynia synagoga zbór\n",
    "stopień_wojskowy: chorąży podporucznik porucznik kapitan major pułkownik generał podpułkownik\n",
    "grzyby_jadalne: pieczarka borowik gąska kurka boczniak kania\n",
    "prądy_filozoficzne: empiryzm stoicyzm racjonalizm egzystencjalizm marksizm romantyzm\n",
    "religie: chrześcijaństwo buddyzm islam prawosławie protestantyzm kalwinizm luteranizm judaizm\n",
    "dzieła_muzyczne: sonata synfonia koncert preludium fuga suita\n",
    "cyfry: jedynka dwójka trójka czwórka piątka szóstka siódemka ósemka dziewiątka\n",
    "owady: ważka biedronka żuk mrówka mucha osa pszczoła chrząszcz\n",
    "broń_biała: miecz topór sztylet nóż siekiera\n",
    "broń_palna: karabin pistolet rewolwer fuzja strzelba\n",
    "komputery: komputer laptop kalkulator notebook\n",
    "kolory: biel żółć czerwień błękit zieleń brąz czerń\n",
    "duchowny: wikary biskup ksiądz proboszcz rabin pop arcybiskup kardynał pastor\n",
    "ryby: karp śledź łosoś dorsz okoń sandacz szczupak płotka\n",
    "napoje_mleczne: jogurt kefir maślanka\n",
    "czynności_sportowe: bieganie skakanie pływanie maszerowanie marsz trucht\n",
    "ubranie:  garnitur smoking frak żakiet marynarka koszula bluzka sweter sweterek sukienka kamizelka spódnica spodnie\n",
    "mebel: krzesło fotel kanapa łóżko wersalka sofa stół stolik ława\n",
    "przestępca: morderca zabójca gwałciciel złodziej bandyta kieszonkowiec łajdak łobuz\n",
    "mięso_wędliny wieprzowina wołowina baranina cielęcina boczek baleron kiełbasa szynka schab karkówka dziczyzna\n",
    "drzewo: dąb klon wiąz jesion świerk sosna modrzew platan buk cis jawor jarzębina akacja\n",
    "źródło_światła: lampa latarka lampka żyrandol żarówka reflektor latarnia lampka\n",
    "organ: wątroba płuco serce trzustka żołądek nerka macica jajowód nasieniowód prostata śledziona\n",
    "oddziały: kompania pluton batalion brygada armia dywizja pułk\n",
    "napój_alkoholowy: piwo wino wódka dżin nalewka bimber wiśniówka cydr koniak wiśniówka\n",
    "kot_drapieżny: puma pantera lampart tygrys lew ryś żbik gepard jaguar\n",
    "metal: żelazo złoto srebro miedź nikiel cyna cynk potas platyna chrom glin aluminium\n",
    "samolot: samolot odrzutowiec awionetka bombowiec myśliwiec samolocik helikopter śmigłowiec\n",
    "owoc: jabłko gruszka śliwka brzoskwinia cytryna pomarańcza grejpfrut porzeczka nektaryna\n",
    "pościel: poduszka prześcieradło kołdra kołderka poduszeczka pierzyna koc kocyk pled\n",
    "agd: lodówka kuchenka pralka zmywarka mikser sokowirówka piec piecyk piekarnik\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in clusters: 328\n",
      "['piśmiennicze', 'pisak', 'flamaster', 'ołówek', 'długopis', 'pióro', 'małe_ssaki', 'mysz', 'szczur', 'chomik', 'łasica', 'kuna', 'bóbr', 'okręty', 'niszczyciel', 'lotniskowiec', 'trałowiec', 'krążownik', 'pancernik', 'fregata', 'korweta', 'lekarze', 'lekarz', 'pediatra', 'ginekolog', 'kardiolog', 'internista', 'geriatra', 'zupy', 'rosół'] ...\n"
     ]
    }
   ],
   "source": [
    "# Cell A: Extract unique words from the clusters\n",
    "def get_unique_words_from_clusters(clusters_txt):\n",
    "    words = []\n",
    "    for line in clusters_txt.split('\\n'):\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        # add the cluster name as well\n",
    "        words.append(parts[0][:-1])\n",
    "        # the cluster name is parts[0], actual words in parts[1:]\n",
    "        cluster_words = parts[1:]\n",
    "        for w in cluster_words:\n",
    "            words.append(w)\n",
    "    return words\n",
    "\n",
    "# We assume \"clusters_txt\" is already defined in the notebook by the evaluation script.\n",
    "all_words = get_unique_words_from_clusters(clusters_txt)\n",
    "print(f\"Total unique words in clusters: {len(all_words)}\")\n",
    "print(all_words[:30], \"...\")\n",
    "all_words = list(set(all_words))  # remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papuga embeddings for ORIGINAL words have been saved to word_embedings_file.txt.\n",
      "Now re-run the evaluation script cell above/below to see the score (Papuga - original).\n"
     ]
    }
   ],
   "source": [
    "# Cell B: Papuga on Original Words\n",
    "with open(\"word_embedings_file_papuga_original.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in all_words:\n",
    "        emb_vec = get_word_embedding_papuga_non_contextual(w)\n",
    "        # Convert to string\n",
    "        vec_str = \" \".join(map(str, emb_vec))\n",
    "        f.write(f\"{w} {vec_str}\\n\")\n",
    "\n",
    "print(\"Papuga embeddings for ORIGINAL words have been saved to word_embedings_file.txt.\")\n",
    "print(\"Now re-run the evaluation script cell above/below to see the score (Papuga - original).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings for ORIGINAL words have been saved to word_embedings_file.txt.\n",
      "Re-run the evaluation script cell to see the score (BERT - original).\n"
     ]
    }
   ],
   "source": [
    "# Cell C: BERT on Original Words\n",
    "with open(\"word_embedings_file_BERT_original.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in all_words:\n",
    "        emb_vec = get_word_embedding_bert_contextual(w)\n",
    "        vec_str = \" \".join(map(str, emb_vec))\n",
    "        f.write(f\"{w} {vec_str}\\n\")\n",
    "\n",
    "print(\"BERT embeddings for ORIGINAL words have been saved to word_embedings_file.txt.\")\n",
    "print(\"Re-run the evaluation script cell to see the score (BERT - original).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papuga embeddings for DISTORTED words have been saved to word_embedings_file.txt.\n",
      "Re-run the evaluation script cell to see the score (Papuga - distorted).\n"
     ]
    }
   ],
   "source": [
    "# Cell D: Papuga on Distorted Words\n",
    "with open(\"word_embedings_file_papuga_deformed.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in all_words:\n",
    "        w_dist = distort_word(w)  # e.g. remove diacritics + random swap\n",
    "        emb_vec = get_word_embedding_papuga_non_contextual(w_dist)\n",
    "        vec_str = \" \".join(map(str, emb_vec))\n",
    "        f.write(f\"{w} {vec_str}\\n\")\n",
    "\n",
    "print(\"Papuga embeddings for DISTORTED words have been saved to word_embedings_file.txt.\")\n",
    "print(\"Re-run the evaluation script cell to see the score (Papuga - distorted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings for DISTORTED words have been saved to word_embedings_file.txt.\n",
      "Re-run the evaluation script cell to see the score (BERT - distorted).\n"
     ]
    }
   ],
   "source": [
    "# Cell E: BERT on Distorted Words\n",
    "with open(\"word_embedings_file_BERT_deformed.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in all_words:\n",
    "        w_dist = distort_word(w)\n",
    "        emb_vec = get_word_embedding_bert_contextual(w_dist)\n",
    "        vec_str = \" \".join(map(str, emb_vec))\n",
    "        # Again, if you want to preserve the *original* key in the script, do:\n",
    "        # f.write(f\"{w} {vec_str}\\n\")\n",
    "        # Otherwise, do:\n",
    "        f.write(f\"{w} {vec_str}\\n\")\n",
    "\n",
    "print(\"BERT embeddings for DISTORTED words have been saved to word_embedings_file.txt.\")\n",
    "print(\"Re-run the evaluation script cell to see the score (BERT - distorted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "university-masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
