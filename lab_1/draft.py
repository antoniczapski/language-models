import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class PapuGaPT2Handler:
    def __init__(self, model_name: str = "flax-community/papuGaPT2"):
        """
        Initializes the tokenizer and the model for papuGaPT2.
        Adds a sliding-window conversation approach.
        Every 3 user questions, a summary block is generated by the same model
        using a special summarization prompt, then appended to the conversation.
        """
        print(f"[INFO] Loading model: {model_name}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

        # If no pad token is defined, add a new one:
        if self.tokenizer.pad_token is None:
            print("[INFO] No pad_token defined. Adding '[PAD]' as the pad token...")
            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            self.model.resize_token_embeddings(len(self.tokenizer))

        # Move model to device (GPU if available)
        self.model.to(self.device)

        # Make sure model is in eval mode
        self.model.eval()

        # List-based conversation history (mix of user messages, assistant messages, and summaries).
        # We start with some example Q&A lines:
        self.conversation_history = [
            "Rozmowa użytkownika z asystentem AI (po polsku).",
            "[Streszczenie wcześniejszych interakcji: Użytkownik zadawał pytania o podstawowe fakty z geografii, historii, nauki i literatury. Asystent udzielił poprawnych i zwięzłych odpowiedzi.]",

            "Użytkownik: Jaka jest stolica Polski?",
            "Asystent: Stolicą Polski jest Warszawa.",

            "Użytkownik: Gdzie znajduje się wieża Eiffla?",
            "Asystent: Wieża Eiffla znajduje się w Paryżu, we Francji.",

            "Użytkownik: Kto napisał 'Lalkę'?",
            "Asystent: Powieść 'Lalka' napisał Bolesław Prus.",
        ]

        # Keep track of how many questions have been asked
        self.user_question_count = 3  # we have 3 user Q in the example

        # We'll keep a maximum number of recent lines (besides summary blocks)
        self.MAX_RECENT_MESSAGES = 6  # keep last 6 user/assistant lines

    def sentence_probability(self, text: str, normalize: bool = True) -> float:
        """
        Estimates the (negative) log-likelihood of a text sequence.
        Optionally normalizes by the number of tokens (True by default).
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=input_ids
            )
            neg_log_likelihood = outputs.loss * input_ids.shape[1]

        if normalize:
            return neg_log_likelihood.item() / input_ids.shape[1]
        else:
            return neg_log_likelihood.item()

    def generate_text(
        self,
        prompt: str,
        max_new_tokens: int = 50,
        num_return_sequences: int = 1,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.95
    ) -> list:
        """
        Generates text continuations from the PapuGaPT2 model given a prompt.
        We use `max_new_tokens` rather than `max_length`.
        """
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=False)
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                num_return_sequences=num_return_sequences,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )

        generated_texts = [
            self.tokenizer.decode(out_id, skip_special_tokens=True)
            for out_id in outputs
        ]
        return generated_texts

    def _build_prompt(self) -> str:
        """
        Builds the final chat prompt from the conversation history.
        Keeps all summary blocks but only the last N user/assistant lines.
        """
        # Keep summary blocks (they start with "[Streszczenie")
        summaries = [line for line in self.conversation_history if line.startswith("[Streszczenie")]
        # Non-summary lines
        normal_lines = [
            line for line in self.conversation_history
            if not line.startswith("[Streszczenie") and not line.startswith("Rozmowa")
        ]
        # Opening line
        opening = [line for line in self.conversation_history if line.startswith("Rozmowa")]

        # Keep only last N lines of normal conversation
        recent_normal = normal_lines[-self.MAX_RECENT_MESSAGES:]

        final_lines = []
        if opening:
            final_lines.extend(opening)
        final_lines.extend(summaries)
        final_lines.extend(recent_normal)

        return "\n".join(final_lines)

    def _build_summarization_prompt(self, lines_to_summarize: list) -> str:
        """
        Builds a custom summarization prompt in Polish, asking the model to produce
        a short (2-3 sentence) summary of the given lines of conversation.
        """
        conversation_text = "\n".join(lines_to_summarize)
        summarization_prompt = (
            "Stwórz krótkie (2-3 zdania) podsumowanie ostatnich interakcji w języku polskim. "
            "Skup się na najważniejszych pytaniach i odpowiedziach, nie podawaj zbędnych szczegółów.\n\n"
            "Oto fragment rozmowy:\n"
            f"{conversation_text}\n\n"
            "Podsumowanie (2-3 zdania):"
        )
        return summarization_prompt

    def _generate_summary(self, lines_for_summary: list) -> str:
        """
        Generates a short summary (2-3 sentences) from the provided lines using the same PapuGaPT2 model.
        Also validates the summary length and tries to shorten it if too long.
        """
        # 1. Build prompt
        summary_prompt = self._build_summarization_prompt(lines_for_summary)

        # 2. Generate the summary (1 candidate is enough)
        candidate_summaries = self.generate_text(
            prompt=summary_prompt,
            max_new_tokens=80,   # up to 80 new tokens
            num_return_sequences=1,
            temperature=0.7,     # a bit lower temperature for summarization
            top_k=50,
            top_p=0.9
        )
        summary_text = candidate_summaries[0]

        # 3. Extract text after "Podsumowanie (2-3 zdania):" if it remains
        if "Podsumowanie (2-3 zdania):" in summary_text:
            summary_text = summary_text.split("Podsumowanie (2-3 zdania):")[-1].strip()

        # 4. Validate length: let's do a simple check on total tokens or characters
        if len(summary_text.split()) > 50:
            # Try a second pass: reduce max_new_tokens or add a more forceful prompt
            shorter_prompt = (
                f"{summary_prompt}\n"
                "Stwórz maksymalnie 30-wyrazowe podsumowanie w 2-3 zdaniach:"
            )
            second_try = self.generate_text(
                prompt=shorter_prompt,
                max_new_tokens=40,
                temperature=0.7,
                top_k=50,
                top_p=0.9,
                num_return_sequences=1
            )
            short_summary = second_try[0]
            if "Stwórz maksymalnie 30-wyrazowe podsumowanie" in short_summary:
                short_summary = short_summary.split("Stwórz maksymalnie 30-wyrazowe podsumowanie")[-1].strip()
            # Use whichever is shorter
            if len(short_summary.split()) < len(summary_text.split()):
                summary_text = short_summary

        return summary_text.strip()

    def _add_summary_block(self) -> None:
        """
        Sums up the last N lines (could be the entire conversation or just the last few).
        Then appends a summary block to the conversation_history.
        Also prints the summary.
        """
        # Let's pick the last 8 normal lines (not including old summary lines) to summarize:
        normal_lines = [
            line for line in self.conversation_history
            if not line.startswith("[Streszczenie") and not line.startswith("Rozmowa")
        ]
        lines_for_summary = normal_lines[-8:]  # summarize the last 8 lines

        summary_text = self._generate_summary(lines_for_summary)
        summary_block = f"[Streszczenie: {summary_text}]"

        # Append to conversation
        self.conversation_history.append(summary_block)

        # Print for user
        print("\n[INFO] Nowe podsumowanie rozmowy:")
        print(summary_block)
        print("-" * 60)

    def get_response(
        self,
        user_question: str,
        max_new_tokens: int = 50,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.95
    ) -> str:
        """
        1. Append "Użytkownik: <question>" to conversation_history.
        2. Build a prompt from the relevant conversation history.
        3. Generate 3 candidate answers.
        4. Choose the best answer (lowest NLL).
        5. Append "Asystent: <best_answer>" to conversation_history.
        6. Every 3 user questions, insert a new summary block (and print it).
        7. Return the new answer in Polish.
        """
        self.user_question_count += 1

        # Add user's new question
        self.conversation_history.append(f"Użytkownik: {user_question}")

        # Build prompt for the assistant
        prompt_before = self._build_prompt() + "\nAsystent:"

        # Generate 3 candidate completions
        candidates = self.generate_text(
            prompt=prompt_before,
            max_new_tokens=max_new_tokens,
            num_return_sequences=3,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p
        )

        # Evaluate each candidate for negative log-likelihood
        per_token_nlls = [
            self.sentence_probability(candidate, normalize=True)
            for candidate in candidates
        ]

        # Pick the best candidate (lowest NLL)
        best_idx = min(range(len(candidates)), key=lambda i: per_token_nlls[i])
        best_candidate = candidates[best_idx]

        # Extract the newly generated portion after "Asystent:"
        if "Asystent:" in best_candidate:
            new_answer = best_candidate.split("Asystent:")[-1].strip()
        else:
            new_answer = best_candidate.strip()

        # Add the final assistant line to conversation
        self.conversation_history.append(f"Asystent: {new_answer}")

        # Every 3 user questions, insert a summary block (and print it)
        if self.user_question_count % 3 == 0:
            self._add_summary_block()

        return new_answer


if __name__ == "__main__":
    """
    Example usage for an interactive chat:
    - A conversation with a sliding history
    - Summaries are generated every 3 user questions with the same model.
    """
    handler = PapuGaPT2Handler()

    print("[INFO] Rozpoczęto konwersację z asystentem AI.\n")
    print("Historia konwersacji (startowa):")
    for line in handler.conversation_history:
        print(line)
    print("-" * 60)

    while True:
        user_input = input("\nTy (Użytkownik): ")
        if user_input.lower() in ["quit", "q", "exit"]:
            print("Zakończono konwersację.")
            break

        assistant_answer = handler.get_response(user_input)
        print(f"Asystent: {assistant_answer}")
