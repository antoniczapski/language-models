import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class LanguageModel:
    def __init__(self, model_name: str = "flax-community/papuGaPT2"):
        """
        Initializes the tokenizer and the model for papuGaPT2.
        Adds a sliding-window conversation approach.
        Every 3 user questions, a summary block is generated by the same model
        using a special summarization prompt, then appended to the conversation.
        """
        print(f"[INFO] Loading model: {model_name}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

        # If no pad token is defined, add a new one:
        if self.tokenizer.pad_token is None:
            print("[INFO] No pad_token defined. Adding '[PAD]' as the pad token...")
            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            self.model.resize_token_embeddings(len(self.tokenizer))

        # Move model to device (GPU if available)
        self.model.to(self.device)

        # Make sure model is in eval mode
        self.model.eval()

        # List-based conversation history (mix of user messages, assistant messages, and summaries).
        # We start with some example Q&A lines:
        self.system_introduction = "Rozmowa użytkownika z asystentem AI (po polsku)."
        self.conversation_history = [
            "[Streszczenie wcześniejszych interakcji: Użytkownik zadawał pytania o podstawowe fakty z geografii, historii, nauki i literatury. Asystent udzielił poprawnych i zwięzłych odpowiedzi.]",

            "Użytkownik: Jaka jest stolica Polski?",
            "Asystent: Stolicą Polski jest Warszawa.",

            "Użytkownik: Gdzie znajduje się wieża Eiffla?",
            "Asystent: Wieża Eiffla znajduje się w Paryżu, we Francji.",

            "Użytkownik: Kto napisał 'Lalkę'?",
            "Asystent: Powieść 'Lalka' napisał Bolesław Prus.",

            "[Streszczenie wcześniejszych interakcji: Użytkownik zadał kilka pytań z dziedziny geografii, historii i nauk przyrodniczych. Asystent dostarczył jasne odpowiedzi dotyczące bitew, planet i zagadnień chemicznych.]",

            "Użytkownik: Kiedy odbyła się Bitwa pod Grunwaldem?",
            "Asystent: Bitwa pod Grunwaldem miała miejsce 15 lipca 1410 roku.",

            "Użytkownik: Jaka planeta jest najbliżej Słońca?",
            "Asystent: Najbliższą planetą Słońca jest Merkury.",

            "Użytkownik: Ile wynosi pierwiastek kwadratowy z 144?",
            "Asystent: Pierwiastek kwadratowy z 144 wynosi 12.",

            "[Streszczenie wcześniejszych interakcji: Rozmowa obejmowała pytania matematyczne, chemiczne oraz ogólne zagadnienia z dziedziny biologii i fizyki. Odpowiedzi były precyzyjne i poprawne.]",

            "Użytkownik: Kto namalował 'Mona Lisę'?",
            "Asystent: Obraz 'Mona Lisa' namalował Leonardo da Vinci.",

            "Użytkownik: Jakie są trzy stany skupienia materii?",
            "Asystent: Trzy podstawowe stany skupienia materii to stały, ciekły i gazowy.",

            "Użytkownik: Co to jest fotosynteza?",
            "Asystent: Fotosynteza to proces, w którym rośliny przekształcają dwutlenek węgla i wodę w glukozę i tlen przy udziale światła słonecznego.",

            "[Streszczenie wcześniejszych interakcji: Dyskusja poruszała zagadnienia z chemii, biologii i sztuki. Asystent odpowiadał krótko i rzeczowo na pytania użytkownika.]",

            "Użytkownik: Jaki jest symbol chemiczny złota?",
            "Asystent: Symbol chemiczny złota to Au.",

            "Użytkownik: Ile nóg ma pająk?",
            "Asystent: Pająk ma osiem nóg.",

            "Użytkownik: Jakie jest najwyższe pasmo górskie na świecie?",
            "Asystent: Najwyższym pasmem górskim na świecie są Himalaje.",

            "[Streszczenie wcześniejszych interakcji: Poruszono tematy związane z chemią, zoologią i geografią fizyczną. Asystent udzielał precyzyjnych odpowiedzi.]"
        ]

        # Keep track of how many questions have been asked
        self.user_question_count = 3  # we have 3 user Q in the example

        # We'll keep a maximum number of recent lines (besides summary blocks)
        self.MAX_RECENT_MESSAGES = 20  # keep last 6 user/assistant lines

    def sentence_probability(self, text: str, normalize: bool = True) -> float:
        """
        Estimates the (negative) log-likelihood of a text sequence.
        Optionally normalizes by the number of tokens (True by default).
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=input_ids
            )
            neg_log_likelihood = outputs.loss * input_ids.shape[1]

        if normalize:
            return neg_log_likelihood.item() / input_ids.shape[1]
        else:
            return neg_log_likelihood.item()

    def generate_text(
        self,
        prompt: str,
        max_new_tokens: int = 50,
        num_return_sequences: int = 1,
        temperature: float = 0.3,
        top_k: int = 50,
        top_p: float = 0.95
    ) -> list:
        """
        Generates text continuations from the PapuGaPT2 model given a prompt.
        We use `max_new_tokens` rather than `max_length`.
        """
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=False)
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                num_return_sequences=num_return_sequences,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )

        generated_texts = [
            self.tokenizer.decode(out_id, skip_special_tokens=True)
            for out_id in outputs
        ]
        return generated_texts

    def _build_prompt(self) -> str:
        """
        Builds the final chat prompt from the conversation history.
        Keeps all summary blocks but only the last N user/assistant lines.
        """
        recent_conversation = self.conversation_history[-min(len(self.conversation_history),self.MAX_RECENT_MESSAGES):]

        return "\n".join([self.system_introduction]+recent_conversation)

    def _build_summarization_prompt(self, lines_to_summarize: list) -> str:
        """
        Builds a custom summarization prompt in Polish, asking the model to produce
        a short (2-3 sentence) summary of the given lines of conversation.
        """
        conversation_text = "\n".join(lines_to_summarize)
        summarization_prompt = (
            "Stwórz krótkie (2-3 zdania) podsumowanie ostatnich interakcji w języku polskim. "
            "Skup się na najważniejszych pytaniach i odpowiedziach, nie podawaj zbędnych szczegółów.\n\n"
            "### Przykład 1:\n"
            "Oto fragment rozmowy:\n"
            "Użytkownik: Jakie są trzy stany skupienia materii?\n"
            "Asystent: Trzy podstawowe stany skupienia materii to stały, ciekły i gazowy.\n"
            "Użytkownik: Co to jest fotosynteza?\n"
            "Asystent: Fotosynteza to proces, w którym rośliny przekształcają dwutlenek węgla i wodę w glukozę i tlen przy udziale światła słonecznego.\n"
            "Użytkownik: Jaki jest symbol chemiczny złota?\n"
            "Asystent: Symbol chemiczny złota to Au.\n"
            "Użytkownik: Ile nóg ma pająk?\n"
            "Asystent: Pająk ma osiem nóg.\n\n"
            "Podsumowanie (2-3 zdania): Użytkownik pytał o stany skupienia materii, proces fotosyntezy, symbol chemiczny złota i liczbę nóg pająka. Asystent udzielił krótkich i precyzyjnych odpowiedzi.\n\n"
            
            "### Przykład 2:\n"
            "Oto fragment rozmowy:\n"
            "Użytkownik: Jaka jest stolica Polski?\n"
            "Asystent: Stolicą Polski jest Warszawa.\n"
            "Użytkownik: Gdzie znajduje się wieża Eiffla?\n"
            "Asystent: Wieża Eiffla znajduje się w Paryżu, we Francji.\n"
            "Użytkownik: Kto napisał 'Lalkę'?\n"
            "Asystent: Powieść 'Lalka' napisał Bolesław Prus.\n"
            "Użytkownik: Jakie jest największe jezioro na świecie?\n"
            "Asystent: Największym jeziorem na świecie pod względem powierzchni jest Morze Kaspijskie.\n\n"
            "Podsumowanie (2-3 zdania): Użytkownik zadawał pytania dotyczące stolicy Polski, położenia wieży Eiffla, autora 'Lalki' oraz największego jeziora na świecie. Asystent odpowiadał krótko i poprawnie.\n\n"
            
            "### Przykład 3:\n"
            "Oto fragment rozmowy:\n"
            "Użytkownik: Kiedy odbyła się Bitwa pod Grunwaldem?\n"
            "Asystent: Bitwa pod Grunwaldem miała miejsce 15 lipca 1410 roku.\n"
            "Użytkownik: Jaka planeta jest najbliżej Słońca?\n"
            "Asystent: Najbliższą planetą Słońca jest Merkury.\n"
            "Użytkownik: Ile wynosi pierwiastek kwadratowy z 144?\n"
            "Asystent: Pierwiastek kwadratowy z 144 wynosi 12.\n"
            "Użytkownik: Kto namalował 'Mona Lisę'?\n"
            "Asystent: Obraz 'Mona Lisa' namalował Leonardo da Vinci.\n\n"
            "Podsumowanie (2-3 zdania): Użytkownik pytał o Bitwę pod Grunwaldem, najbliższą planetę Słońca, pierwiastek kwadratowy z 144 oraz autora obrazu 'Mona Lisa'. Asystent dostarczył konkretne odpowiedzi.\n\n"
            
            "### Przykład 4:\n"
            "Oto fragment rozmowy:\n"
            f"{conversation_text}\n\n"
            "Podsumowanie (2-3 zdania):"
        )

        return summarization_prompt

    def _generate_summary(self, lines_for_summary: list) -> str:
        """
        Generates a short summary (2-3 sentences) from the provided lines using the same PapuGaPT2 model.
        Also validates the summary length and tries to shorten it if too long.
        """
        # 1. Build prompt
        summary_prompt = self._build_summarization_prompt(lines_for_summary)

        # 2. Generate the summary (1 candidate is enough)
        candidate_summaries = self.generate_text(
            prompt=summary_prompt,
            max_new_tokens=80,   # up to 80 new tokens
            num_return_sequences=1,
            temperature=0.3,     # a bit lower temperature for summarization
            top_k=50,
            top_p=0.9
        )
        summary_text = candidate_summaries[0].split(summary_prompt)[-1]

        # 3. Extract text after "Podsumowanie (2-3 zdania):" if it remains
        summary_text = summary_text.split('\n')[0]
        return summary_text

    def _add_summary_block(self) -> None:
        """
        Sums up the last N lines (could be the entire conversation or just the last few).
        Then appends a summary block to the conversation_history.
        Also prints the summary.
        """
        # Let's pick the last 8 normal lines (not including old summary lines) to summarize:
        normal_lines = [
            line for line in self.conversation_history
            if not line.startswith("[Streszczenie") and not line.startswith("Rozmowa")
        ]
        lines_for_summary = normal_lines[-8:]  # summarize the last 8 lines

        summary_text = self._generate_summary(lines_for_summary)
        summary_block = f"[Streszczenie wcześniejszych interakcji: {summary_text}]"

        # Append to conversation
        self.conversation_history.append(summary_block)

        # Print for user
        print(summary_block)
        
    def get_response(
        self,
        user_question: str,
        max_new_tokens: int = 50,
        temperature: float = 0.3,
        top_k: int = 50,
        top_p: float = 0.95
    ) -> str:
        """
        1. Append "Użytkownik: <question>" to conversation_history.
        2. Build a prompt from the relevant conversation history.
        3. Generate 3 candidate answers.
        4. Choose the best answer (lowest NLL).
        5. Append "Asystent: <best_answer>" to conversation_history.
        6. Every 3 user questions, insert a new summary block (and print it).
        7. Return the new answer in Polish.
        """
        self.user_question_count += 1

        # Add user's new question
        self.conversation_history.append(f"Użytkownik: {user_question}")

        # Build prompt for the assistant
        prompt_before = self._build_prompt() + "\nAsystent:"

        # Generate 3 candidate completions
        candidates = [self.generate_text(
            prompt=prompt_before,
            max_new_tokens=max_new_tokens,
            num_return_sequences=1,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p
        )[0].split(prompt_before)[-1] for _ in range(3)]

        # Evaluate each candidate for negative log-likelihood
        per_token_nlls = [
            self.sentence_probability(candidate, normalize=True)
            for candidate in candidates
        ]

        # Pick the best candidate (lowest NLL)
        best_idx = min(range(len(candidates)), key=lambda i: per_token_nlls[i])
        best_candidate = candidates[best_idx]

        # Extract the newly generated portion after "Asystent:"
        new_answer = best_candidate.split('\n')[0].strip()

        # Add the final assistant line to conversation
        self.conversation_history.append(f"Asystent: {new_answer}")

        print(f"Asystent: {new_answer}")
        # Every 3 user questions, insert a summary block (and print it)
        if self.user_question_count % 3 == 0:
            self._add_summary_block()

        return new_answer


if __name__ == "__main__":
    """
    Example usage for an interactive chat:
    - A conversation with a sliding history
    - Summaries are generated every 3 user questions with the same model.
    """
    # model = "flax-community/papuGaPT2"
    model = "eryk-mazus/polka-1.1b"
    handler = LanguageModel(model_name=model)

    print("[INFO] Rozpoczęto konwersację z asystentem AI.\n")
    print("Historia konwersacji (startowa):")
    for line in handler.conversation_history:
        print(line)
    print("-" * 60)

    while True:
        try:
            user_input = input("\nTy (Użytkownik): ")
        except KeyboardInterrupt:
            print("\n[INFO] Koniec konwersacji.")
            break
        
        handler.get_response(user_input)
        
