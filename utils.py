import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
from typing import List, Tuple
import re
from tqdm import tqdm
import torch.nn.functional as F

class LanguageModel:
    def __init__(self, model_name: str = "eryk-mazus/polka-1.1b"):
        """
        Initializes the tokenizer and the model for papuGaPT2.
        Adds a sliding-window conversation approach.
        Every 3 user questions, a summary block is generated by the same model
        using a special summarization prompt, then appended to the conversation.
        """
        print(f"[INFO] Loading model: {model_name}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"[INFO] Using device: {self.device}")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

        # If no pad token is defined, add a new one:
        if self.tokenizer.pad_token is None:
            print("[INFO] No pad_token defined. Adding '[PAD]' as the pad token...")
            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            self.model.resize_token_embeddings(len(self.tokenizer))

        # Move model to device (GPU if available)
        self.model.to(self.device)

        # Make sure model is in eval mode
        self.model.eval()

    def sentence_probability(self, text: str, normalize: bool = True) -> float:
        """
        Estimates the (negative) log-likelihood of a text sequence.
        Optionally normalizes by the number of tokens (True by default).
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=input_ids
            )
            neg_log_likelihood = outputs.loss * input_ids.shape[1]

        if normalize:
            return neg_log_likelihood.item() / input_ids.shape[1]
        else:
            return neg_log_likelihood.item()

    def generate_text(
        self,
        prompt: str,
        max_new_tokens: int = 50,
        temperature: float = 0.3,
        top_k: int = 50,
        top_p: float = 0.95
    ) -> list:
        """
        Generates text continuations from the PapuGaPT2 model given a prompt.
        We use `max_new_tokens` rather than `max_length`.
        """
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=False)
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            if temperature == 0.0:
                do_sample=False
                outputs = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=input_ids.shape[1] + max_new_tokens,
                    num_return_sequences=1,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            else:
                outputs = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=max_new_tokens,
                    num_return_sequences=1,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id
                )

        generated_texts = [
            self.tokenizer.decode(out_id, skip_special_tokens=True)
            for out_id in outputs
        ]
        return generated_texts[0].split(prompt)[-1].strip()
    
    def generate_text_with_allowed_tokens(
        self,
        prompt: str,
        allowed_tokens: list[str],
        max_new_tokens: int = 50,
        temperature: float = 1.0
    ) -> str:
        """
        Generates text from the language model, but restricts sampling
        to only those tokens present in 'allowed_tokens'.
        
        :param prompt:          The input prompt string.
        :param allowed_tokens:  A list of strings, each assumed to be 
                                a single token or symbol. Only these 
                                tokens will have non-zero probability.
        :param max_new_tokens:  Maximum tokens to generate.
        :param temperature:     Sampling temperature.
        :return: The newly generated text (excluding the original prompt).
        """
        # Convert the prompt to input_ids
        inputs = self.tokenizer(prompt, return_tensors="pt")
        input_ids = inputs["input_ids"].to(self.device)
        
        # Build a set of allowed token IDs
        allowed_ids = set()
        for tok in allowed_tokens:
            # We assume each 'tok' is basically one token. If multiple tokens, you need more logic.
            enc = self.tokenizer.encode(tok, add_special_tokens=False)
            for e in enc:
                allowed_ids.add(e)
        
        generated = input_ids.clone()  # we keep track of the growing sequence
        
        for _ in range(max_new_tokens):
            # Forward pass
            outputs = self.model(generated)
            logits = outputs.logits[:, -1, :]  # shape: [batch=1, vocab_size]

            # Mask out disallowed tokens by assigning -inf to their logits
            # We'll do the inverse: set everything to -inf, then restore allowed tokens
            new_logits = torch.full_like(logits, float('-inf'))
            # Only keep allowed IDs
            for tid in allowed_ids:
                new_logits[0, tid] = logits[0, tid]
            
            # Apply temperature
            if temperature > 0:
                new_logits = new_logits / temperature

            # Convert to probabilities
            probs = F.softmax(new_logits, dim=-1)

            # Sample (or pick argmax if temperature=0)
            next_token = torch.multinomial(probs, num_samples=1)

            # Append next token
            generated = torch.cat([generated, next_token], dim=1)

            # Optional: break if we hit special tokens like EOS
            # if next_token.item() == self.tokenizer.eos_token_id:
            #     break

        # Decode the entire sequence
        full_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)
        
        # Return only the newly generated text after the prompt
        # (strip prompt from the front).
        # One simple approach:
        new_part = full_text[len(prompt):].strip()
        return new_part

def load_qa_pairs(questions_file: str, answers_file: str) -> List[Tuple[str, List[str]]]:
    """
    Loads questions and their corresponding answers from the provided text files.

    :param questions_file: Path to the questions.txt file.
    :param answers_file: Path to the answers.txt file.
    :return: A list of tuples where each tuple contains a question and a list of its answers.
    """
    if not os.path.isfile(questions_file):
        raise FileNotFoundError(f"Questions file not found: {questions_file}")
    if not os.path.isfile(answers_file):
        raise FileNotFoundError(f"Answers file not found: {answers_file}")

    with open(questions_file, 'r', encoding='utf-8') as qf:
        questions = [line.strip() for line in qf if line.strip()]
    
    with open(answers_file, 'r', encoding='utf-8') as af:
        answers = [line.strip() for line in af if line.strip()]
    
    if len(questions) != len(answers):
        raise ValueError("The number of questions and answers must be the same.")

    qa_pairs = []
    for q, a in zip(questions, answers):
        # Split multiple answers separated by tabs
        answer_list = [ans.strip().lower() for ans in a.split('\t') if ans.strip()]
        qa_pairs.append((q, answer_list))
    
    return qa_pairs


def evaluate_model(qa_pairs: List[Tuple[str, List[str]]], generate_answer_func) -> float:
    """
    Evaluates the language model's performance on the given question-answer pairs.

    :param qa_pairs: List of tuples containing questions and their list of correct answers.
    :param handler: An instance of the language model handler with `sentence_probability` method.
    :param generate_answer_func: Function to generate an answer given a question.
    :param normalize: Whether to normalize text (e.g., lowercase) before comparison.
    :return: Accuracy as a float representing the proportion of correct answers.
    """
    correct = 0
    total = len(qa_pairs)

    for idx, (question, correct_answers) in tqdm(enumerate(qa_pairs, 1)):
        # Generate answer using the provided function
        generated_answer = generate_answer_func(question)
        
        # Normalize answers if required
        generated_answer_norm = generated_answer.strip().lower()
        correct_answers_norm = [ans.lower() for ans in correct_answers]
        
        # Check if generated answer is among the correct answers
        if generated_answer_norm in correct_answers_norm:
            correct += 1
            result = "Correct"
        else:
            result = "Incorrect"
        
        # print(f"Q{idx}: {question}")
        # print(f"Generated Answer: {generated_answer}")
        # print(f"Expected Answers: {', '.join(correct_answers)}")
        # print(f"Result: {result}\n")
    
    accuracy = correct / total if total > 0 else 0.0
    print(f"Evaluation Complete: {correct}/{total} correct answers.")
    print(f"Accuracy: {accuracy*100:.2f}%")
    return accuracy
