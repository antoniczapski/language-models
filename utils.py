import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class LanguageModel:
    def __init__(self, model_name: str = "eryk-mazus/polka-1.1b"):
        """
        Initializes the tokenizer and the model for papuGaPT2.
        Adds a sliding-window conversation approach.
        Every 3 user questions, a summary block is generated by the same model
        using a special summarization prompt, then appended to the conversation.
        """
        print(f"[INFO] Loading model: {model_name}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"[INFO] Using device: {self.device}")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

        # If no pad token is defined, add a new one:
        if self.tokenizer.pad_token is None:
            print("[INFO] No pad_token defined. Adding '[PAD]' as the pad token...")
            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            self.model.resize_token_embeddings(len(self.tokenizer))

        # Move model to device (GPU if available)
        self.model.to(self.device)

        # Make sure model is in eval mode
        self.model.eval()

    def sentence_probability(self, text: str, normalize: bool = True) -> float:
        """
        Estimates the (negative) log-likelihood of a text sequence.
        Optionally normalizes by the number of tokens (True by default).
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=input_ids
            )
            neg_log_likelihood = outputs.loss * input_ids.shape[1]

        if normalize:
            return neg_log_likelihood.item() / input_ids.shape[1]
        else:
            return neg_log_likelihood.item()

    def generate_text(
        self,
        prompt: str,
        max_new_tokens: int = 50,
        temperature: float = 0.3,
        top_k: int = 50,
        top_p: float = 0.95
    ) -> list:
        """
        Generates text continuations from the PapuGaPT2 model given a prompt.
        We use `max_new_tokens` rather than `max_length`.
        """
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=False)
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                num_return_sequences=1,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )

        generated_texts = [
            self.tokenizer.decode(out_id, skip_special_tokens=True)
            for out_id in outputs
        ]
        return generated_texts.split(prompt)[-1]
